{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of AfterWork_Essentials_Notebook_Data_Wrangling_with_Python","provenance":[{"file_id":"1HeVXPDh09N6SBwNXqpLrWGcf20UEkQ5j","timestamp":1618414220928}],"collapsed_sections":["pcxusFFvAC4A","oZqlt-RYAD9W","4c8lbnCpAKR7","8tAUAu98l-E6","o39LEL0PmKJa","2gjU-yrmmo8I","OWcy51lCOuTv","ONPG_rNpOuT8","zgSTFXdcOuUc","ebaricL9O20B","WWZ0GCbCO20L","XXsiMoefO21J","UQvS83K_PBJh","CjklXwZLPBJw","qP7c_QZcPBK0","jmQzRWnBAUip","RVDxacbDW-Wo","PxBhzro9WOsp","SZOg8AbeAaub","FFOkaq3vXZMh","-1tL9P5BAhWS","-DeWc_D7WVBl","yVTVl_GpWToR","bISArgNWBxR-","Hf72T4GKXzb_","v5M3iGK8PZ3Y","jnxNSUhdPZ3u","wt8Se1hXPZ4C","vAuIYctBB1ZX","fJ84H5gZYIgZ"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1f9HbIqC9Glg"},"source":["<font color=\"blue\">To use this notebook on Colaboratory, you will need to make a copy of it. Go to File > Save a Copy in Drive. You can then use the new copy that will appear in the new tab.</font>\n"]},{"cell_type":"markdown","metadata":{"id":"e6Z5SVjn9XxQ"},"source":["# Essentials Notebook: Data Wrangling with Python"]},{"cell_type":"markdown","metadata":{"id":"pcxusFFvAC4A"},"source":["## Pre-requisites"]},{"cell_type":"code","metadata":{"id":"_-wn-gNt8_0R"},"source":["# Pre-requisite 1\n","# ---\n","# The first thing that we will do in this notebook is \n","# to import the pandas library for data manipulation.\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nj8z63_CMpmb"},"source":["# Pre-requisite 2\n","# ---\n","# We will also import the numpy library which\n","# will allow us to perform scientific computations.\n","# ---\n","# For those who are not familiar with what a library is,\n","# A library is a collection of related pieces of code that \n","# have been compiled and stored together for reuse.\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZqlt-RYAD9W"},"source":["## 1. Reading Data"]},{"cell_type":"code","metadata":{"id":"GwZMMVOTAHbV","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618414844915,"user_tz":-180,"elapsed":4457,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"27c49bad-e308-4f7b-a2fe-c82208de06df"},"source":["# Example 1\n","# --- \n","# Loading a dataset (csv file) from a url \n","# ---\n","# Dataset url (csv file) = http://bit.ly/IrisDataset\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","import pandas as pd\n","\n","# Reading our csv file from the given url and storing it to a dataframe.\n","# A data frame is a two-dimensional data structure that is used to \n","# represent a table of data with rows and columns.\n","# ---\n","#\n","df = pd.read_csv(\"http://bit.ly/IrisDataset\")\n","\n","# Previewing the first 5 records\n","df.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal_length  sepal_width  petal_length  petal_width      species\n","0           5.1          3.5           1.4          0.2  Iris-setosa\n","1           4.9          3.0           1.4          0.2  Iris-setosa\n","2           4.7          3.2           1.3          0.2  Iris-setosa\n","3           4.6          3.1           1.5          0.2  Iris-setosa\n","4           5.0          3.6           1.4          0.2  Iris-setosa"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"9o_jT5Uyqx1G","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618415170010,"user_tz":-180,"elapsed":2203,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"d58b7b1e-ff38-477f-b7ea-cee30d79151b"},"source":["# Example 2\n","# ---\n","# We can also load A CSV Into pandas as shown below\n","# ---\n","# Dataset url = http://bit.ly/CitiesDataset1\n","# ----\n","# Instructions:\n","# 1. Visit the above dataset url with a browser. Then download the file.\n","# 2. Within this notebook you will need to have the Table of contents left sidebar open.\n","#    If its not open, you can open the sidebar clicking at the top of the notebook View -> Table of Contents.\n","# 3. Once its open click on the Files tab within the sidebar (this is on the farmost right).\n","# 4. Then upload the downloaded file to this location/ Or drag the file\n","# 5. Once uploaded you will use the name of the file to open the file as shown below. \n","#    You can also store in a directory/folder then reference that directory while reading the file.\n","#    i.e. the file cities stored in a directory named finance, then the reference pd.read_csv(\"/finance/cities.csv\")\n","# ---\n","# \n","import pandas as pd\n","# Let's read the cities csv file\n","df_cities = pd.read_csv(\"cities.csv\")\n","\n","# Previewing the first five records\n","df_cities.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>temperature</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Aalborg</td>\n","      <td>Denmark</td>\n","      <td>57.03</td>\n","      <td>9.92</td>\n","      <td>7.52</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Aberdeen</td>\n","      <td>United Kingdom</td>\n","      <td>57.17</td>\n","      <td>-2.08</td>\n","      <td>8.10</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Abisko</td>\n","      <td>Sweden</td>\n","      <td>63.35</td>\n","      <td>18.83</td>\n","      <td>0.20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Adana</td>\n","      <td>Turkey</td>\n","      <td>36.99</td>\n","      <td>35.32</td>\n","      <td>18.67</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Albacete</td>\n","      <td>Spain</td>\n","      <td>39.00</td>\n","      <td>-1.87</td>\n","      <td>12.62</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       city         country  latitude  longitude  temperature\n","0   Aalborg         Denmark     57.03       9.92         7.52\n","1  Aberdeen  United Kingdom     57.17      -2.08         8.10\n","2    Abisko          Sweden     63.35      18.83         0.20\n","3     Adana          Turkey     36.99      35.32        18.67\n","4  Albacete           Spain     39.00      -1.87        12.62"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"EUoQlxZpAM3Q","colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"status":"ok","timestamp":1618416233687,"user_tz":-180,"elapsed":1196,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"ae89103d-0238-4db8-c0a2-e9c0879b89d0"},"source":["# Challenge 1a\n","# ---\n","# Question: Load the following hotels dataset and preview the first 5 records.\n","# Hint: Use a different dataframe that the one used in the example dataset \n","#       i.e. using the dataframe name \"hotel_df\" rather using the dataframe name \"df\"\n","#       If you use the same dataframe names for the examples and challenges you \n","#       might experience a crash.\n","#       We will be using this dataset for most of the challenges.\n","#     : To work quickly on theses challenges, copy paste example code and modify the your need. \n","# ---\n","# Dataset url (csv file) = https://bit.ly/HotelBookingsDB\n","# This data set contains booking information for a city hotel and a resort hotel, \n","# and includes information such as when the booking was made, length of stay, \n","# the number of adults, children, and/or babies, and the number of available \n","# parking spaces, among other things.\n","# ---\n","# OUR CODE GOES BELOW\n","#\n","import pandas as pd\n","# Let's read the cities csv file\n","hotel_df = pd.read_csv(\"hotel_bookings.csv\")\n","\n","# Previewing the first five records\n","hotel_df.tail()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hotel</th>\n","      <th>is_canceled</th>\n","      <th>lead_time</th>\n","      <th>arrival_date_year</th>\n","      <th>arrival_date_month</th>\n","      <th>arrival_date_week_number</th>\n","      <th>arrival_date_day_of_month</th>\n","      <th>stays_in_weekend_nights</th>\n","      <th>stays_in_week_nights</th>\n","      <th>adults</th>\n","      <th>children</th>\n","      <th>babies</th>\n","      <th>meal</th>\n","      <th>country</th>\n","      <th>market_segment</th>\n","      <th>distribution_channel</th>\n","      <th>is_repeated_guest</th>\n","      <th>previous_cancellations</th>\n","      <th>previous_bookings_not_canceled</th>\n","      <th>reserved_room_type</th>\n","      <th>assigned_room_type</th>\n","      <th>booking_changes</th>\n","      <th>deposit_type</th>\n","      <th>agent</th>\n","      <th>company</th>\n","      <th>days_in_waiting_list</th>\n","      <th>customer_type</th>\n","      <th>adr</th>\n","      <th>required_car_parking_spaces</th>\n","      <th>total_of_special_requests</th>\n","      <th>reservation_status</th>\n","      <th>reservation_status_date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>119385</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>23</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>30</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>BEL</td>\n","      <td>Offline TA/TO</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>A</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>394.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>96.14</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-06</td>\n","    </tr>\n","    <tr>\n","      <th>119386</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>102</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>31</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>FRA</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>E</td>\n","      <td>E</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>225.43</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","    <tr>\n","      <th>119387</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>34</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>31</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>DEU</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>D</td>\n","      <td>D</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>157.71</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","    <tr>\n","      <th>119388</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>109</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>31</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>GBR</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>A</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>89.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>104.40</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","    <tr>\n","      <th>119389</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>205</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>29</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>HB</td>\n","      <td>DEU</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>A</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>151.20</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             hotel  is_canceled  ...  reservation_status  reservation_status_date\n","119385  City Hotel            0  ...           Check-Out               2017-09-06\n","119386  City Hotel            0  ...           Check-Out               2017-09-07\n","119387  City Hotel            0  ...           Check-Out               2017-09-07\n","119388  City Hotel            0  ...           Check-Out               2017-09-07\n","119389  City Hotel            0  ...           Check-Out               2017-09-07\n","\n","[5 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"_Y1poor54Wa8","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618416401871,"user_tz":-180,"elapsed":1516,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"f159b603-0e1a-45b8-f508-cd98b2ccf694"},"source":["# Challenge 1b\n","# ---\n","# Question: Load a dataset (excel file) from the url below\n","# Hint: - Use the read_excel() function rather than using the read_csv() function\n","#       - Use a new dataframe windmill_df\n","# ---\n","# Dataset url = http://bit.ly/WindmillDataset\n","# ---\n","# OUR CODE GOES BELOW\n","#\n","import pandas as pd\n","# Let's read the cities csv file\n","df = pd.read_excel(\"Windmill Data.xlsx\")\n","\n","# Previewing the first five records\n","windmill_df.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Windmill</th>\n","      <th>Wind Speed (m/s)</th>\n","      <th>Power output (MW)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>a1</td>\n","      <td>1.096875</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>a2</td>\n","      <td>1.231528</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>a3</td>\n","      <td>1.275139</td>\n","      <td>0.005479</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>a4</td>\n","      <td>1.365486</td>\n","      <td>0.010104</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>a5</td>\n","      <td>1.387778</td>\n","      <td>0.010812</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Windmill  Wind Speed (m/s)  Power output (MW)\n","0       a1          1.096875           0.000000\n","1       a2          1.231528           0.000000\n","2       a3          1.275139           0.005479\n","3       a4          1.365486           0.010104\n","4       a5          1.387778           0.010812"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"4c8lbnCpAKR7"},"source":["## 2. Data Exploration"]},{"cell_type":"code","metadata":{"id":"CsUVqRqbATr0"},"source":["# Example 2a\n","# ---\n","# Determining the no. of records in the dataset \n","# NB: We will use the above loaded dataset in example 1\n","# ---\n","# OUR CODE GOES BELOW\n","#\n","\n","# Shape returns no. or records/instances (left) and columns/variables (right)\n","#\n","df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RvZTev3C37-2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618416822970,"user_tz":-180,"elapsed":1314,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"84585111-6cd1-40b1-ff46-c76b17f2ec78"},"source":["# Challenge 2a\n","# ---\n","# Question: Determine the no. of records in the our hotels dataset.\n","# NB: last() gives us the last 5 records/instances\n","# ---  \n","# OUR CODE GOES BELOW\n","#\n","import pandas as pd\n","# Let's read the cities csv file\n","df = pd.read_csv(\"hotel_bookings.csv\")\n","\n","# Previewing the first five records\n","df.shape"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(119390, 32)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"sWVV3zfwGgHK"},"source":["# Example 2b\n","# ---\n","# Previewing the last few records/instances of our dataset\n","# ---\n","# OUR CODE GOES BELOW\n","#\n","df.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s96T5zy34jZh","colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"status":"ok","timestamp":1618416899835,"user_tz":-180,"elapsed":1264,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"5b7c1802-2a10-42e9-d5bd-379837c921a7"},"source":["# Challenge 2c\n","# ---\n","# Question: Preview the last few records in the hotels dataset.\n","# Hint = Use the dataframe you created for this dataset. \n","# ---\n","# OUR CODE GOES BELOW\n","#\n","\n","import pandas as pd\n","# Let's read the cities csv file\n","df = pd.read_csv(\"hotel_bookings.csv\")\n","\n","# Previewing the first five records\n","df.tail()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hotel</th>\n","      <th>is_canceled</th>\n","      <th>lead_time</th>\n","      <th>arrival_date_year</th>\n","      <th>arrival_date_month</th>\n","      <th>arrival_date_week_number</th>\n","      <th>arrival_date_day_of_month</th>\n","      <th>stays_in_weekend_nights</th>\n","      <th>stays_in_week_nights</th>\n","      <th>adults</th>\n","      <th>children</th>\n","      <th>babies</th>\n","      <th>meal</th>\n","      <th>country</th>\n","      <th>market_segment</th>\n","      <th>distribution_channel</th>\n","      <th>is_repeated_guest</th>\n","      <th>previous_cancellations</th>\n","      <th>previous_bookings_not_canceled</th>\n","      <th>reserved_room_type</th>\n","      <th>assigned_room_type</th>\n","      <th>booking_changes</th>\n","      <th>deposit_type</th>\n","      <th>agent</th>\n","      <th>company</th>\n","      <th>days_in_waiting_list</th>\n","      <th>customer_type</th>\n","      <th>adr</th>\n","      <th>required_car_parking_spaces</th>\n","      <th>total_of_special_requests</th>\n","      <th>reservation_status</th>\n","      <th>reservation_status_date</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>119385</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>23</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>30</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>BEL</td>\n","      <td>Offline TA/TO</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>A</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>394.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>96.14</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-06</td>\n","    </tr>\n","    <tr>\n","      <th>119386</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>102</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>31</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>FRA</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>E</td>\n","      <td>E</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>225.43</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","    <tr>\n","      <th>119387</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>34</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>31</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>DEU</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>D</td>\n","      <td>D</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>157.71</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","    <tr>\n","      <th>119388</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>109</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>31</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>BB</td>\n","      <td>GBR</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>A</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>89.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>104.40</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","    <tr>\n","      <th>119389</th>\n","      <td>City Hotel</td>\n","      <td>0</td>\n","      <td>205</td>\n","      <td>2017</td>\n","      <td>August</td>\n","      <td>35</td>\n","      <td>29</td>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>HB</td>\n","      <td>DEU</td>\n","      <td>Online TA</td>\n","      <td>TA/TO</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>A</td>\n","      <td>A</td>\n","      <td>0</td>\n","      <td>No Deposit</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>Transient</td>\n","      <td>151.20</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Check-Out</td>\n","      <td>2017-09-07</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             hotel  is_canceled  ...  reservation_status  reservation_status_date\n","119385  City Hotel            0  ...           Check-Out               2017-09-06\n","119386  City Hotel            0  ...           Check-Out               2017-09-07\n","119387  City Hotel            0  ...           Check-Out               2017-09-07\n","119388  City Hotel            0  ...           Check-Out               2017-09-07\n","119389  City Hotel            0  ...           Check-Out               2017-09-07\n","\n","[5 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"0snrMicC4urE"},"source":["# Challenge 2d\n","# ---\n","# Question: Preview a sample of 10 records from the hotels dataset.\n","# Hint: Use the dataframe you created for this dataset. \n","#     : Use the sample() function, and add no. of desired records as the parameter.\n","# ---\n","# OUR CODE GOES BELOW\n","#"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dx2kj5ZHGu-y"},"source":["# Example 2c\n","# ---\n","# Checking the datatypes of df variables (columns)\n","# --- \n","# OUR CODE GOES BELOW\n","#\n","df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3koGy_RATgd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618416940381,"user_tz":-180,"elapsed":1518,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"28317400-7b96-4d3c-d4db-69f2b37fd1c3"},"source":["# Challenge 2e\n","# ---\n","# Question: Check the datatypes of the hotels dataset.\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","import pandas as pd\n","# Let's read the cities csv file\n","df = pd.read_csv(\"hotel_bookings.csv\")\n","\n","# Previewing the first five records\n","df.dtypes"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["hotel                              object\n","is_canceled                         int64\n","lead_time                           int64\n","arrival_date_year                   int64\n","arrival_date_month                 object\n","arrival_date_week_number            int64\n","arrival_date_day_of_month           int64\n","stays_in_weekend_nights             int64\n","stays_in_week_nights                int64\n","adults                              int64\n","children                          float64\n","babies                              int64\n","meal                               object\n","country                            object\n","market_segment                     object\n","distribution_channel               object\n","is_repeated_guest                   int64\n","previous_cancellations              int64\n","previous_bookings_not_canceled      int64\n","reserved_room_type                 object\n","assigned_room_type                 object\n","booking_changes                     int64\n","deposit_type                       object\n","agent                             float64\n","company                           float64\n","days_in_waiting_list                int64\n","customer_type                      object\n","adr                               float64\n","required_car_parking_spaces         int64\n","total_of_special_requests           int64\n","reservation_status                 object\n","reservation_status_date            object\n","dtype: object"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"8tAUAu98l-E6"},"source":["## 3. Standardisation"]},{"cell_type":"markdown","metadata":{"id":"o39LEL0PmKJa"},"source":["#### <font color=\"blue\">Examples</font>"]},{"cell_type":"code","metadata":{"id":"Ql23JRMAlvv2","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618417005430,"user_tz":-180,"elapsed":3709,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"c88fb1c4-7345-4114-d568-0d81c9183e80"},"source":["# Example 1\n","# --- \n","# Renaming column names\n","# ---\n","# Dataset url = http://bit.ly/DataCleaningDataset\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","# Reading our dataset from the url \n","# ---\n","# \n","df = pd.read_csv('http://bit.ly/DataCleaningDataset')\n","df.head()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NAME;CITY;COUNTRY;HEIGHT;WEIGHT;ACCOUNT A;ACCOUNT B;TOTAL ACCOUNT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako ;LISBON    ;PORTUGAL     ;56;132;2390...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul;LONDON   ;UNITED KINGDOM;62;165;4500...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules;Stockholm;Sweden;48;117;;5504;8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels;BRUSSELS;BELGIUM;59;121;4344...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark;  Berlin;GERMANYY;53;126;7000;19...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  NAME;CITY;COUNTRY;HEIGHT;WEIGHT;ACCOUNT A;ACCOUNT B;TOTAL ACCOUNT\n","0  Adi Dako ;LISBON    ;PORTUGAL     ;56;132;2390...               \n","1  John Paul;LONDON   ;UNITED KINGDOM;62;165;4500...               \n","2     Cindy Jules;Stockholm;Sweden;48;117;;5504;8949               \n","3      Arthur Kegels;BRUSSELS;BELGIUM;59;121;4344...               \n","4  Freya Bismark;  Berlin;GERMANYY;53;126;7000;19...               "]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"BkKMMGbdztE7","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618417041186,"user_tz":-180,"elapsed":3459,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"03528a74-ecc7-4c5e-fa6e-799437ec72b0"},"source":["# We specify the character ; as our separator so that we can \n","# be able to ready the above file that has \";\" as a separator \n","# for our columns. We should note that many dataset may note have\n","# such a structure, but if we come across this kind of a scenario\n","# this is how we would read our file.\n","# ---\n","# \n","df = pd.read_csv('http://bit.ly/DataCleaningDataset', ';')\n","df.head()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NAME</th>\n","      <th>CITY</th>\n","      <th>COUNTRY</th>\n","      <th>HEIGHT</th>\n","      <th>WEIGHT</th>\n","      <th>ACCOUNT A</th>\n","      <th>ACCOUNT B</th>\n","      <th>TOTAL ACCOUNT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>LISBON</td>\n","      <td>PORTUGAL</td>\n","      <td>56</td>\n","      <td>132.0</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>LONDON</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>62</td>\n","      <td>165.0</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>Stockholm</td>\n","      <td>Sweden</td>\n","      <td>48</td>\n","      <td>117.0</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>BRUSSELS</td>\n","      <td>BELGIUM</td>\n","      <td>59</td>\n","      <td>121.0</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>Berlin</td>\n","      <td>GERMANYY</td>\n","      <td>53</td>\n","      <td>126.0</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                NAME        CITY  ... ACCOUNT B  TOTAL ACCOUNT\n","0          Adi Dako   LISBON      ...      4340           6730\n","1          John Paul   LONDON     ...     34334          38834\n","2        Cindy Jules   Stockholm  ...      5504           8949\n","3      Arthur Kegels    BRUSSELS  ...      8999            300\n","4      Freya Bismark      Berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"tjbZr_6MXhLb","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618417346362,"user_tz":-180,"elapsed":1527,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"4f97bcfd-4114-4a9a-c64f-c62f695d3314"},"source":["# Example 1a\n","# --- \n","# We now rename our columns.\n","# We can use this method if we have many column names.\n","# We will use the str.strip(), str.lower(), str.replace() functions \n","# to ensure that our column names are in lowercase format that we easily \n","# reference while performing further analysis.\n","# ---\n","# str.strip() - This fuction is used to remove leading and trailing characters.\n","# str.lower() - This function is used to convert all characters to lowercase\n","# str.replace() - This fuction is used to replace text with some other text.\n","# ---\n","#\n","df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n","\n","# Then preview our resulting dataframe\n","# ---\n","df.head()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>account_a</th>\n","      <th>account_b</th>\n","      <th>total_account</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>LISBON</td>\n","      <td>PORTUGAL</td>\n","      <td>56</td>\n","      <td>132.0</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>LONDON</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>62</td>\n","      <td>165.0</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>Stockholm</td>\n","      <td>Sweden</td>\n","      <td>48</td>\n","      <td>117.0</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>BRUSSELS</td>\n","      <td>BELGIUM</td>\n","      <td>59</td>\n","      <td>121.0</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>Berlin</td>\n","      <td>GERMANYY</td>\n","      <td>53</td>\n","      <td>126.0</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                name        city  ... account_b  total_account\n","0          Adi Dako   LISBON      ...      4340           6730\n","1          John Paul   LONDON     ...     34334          38834\n","2        Cindy Jules   Stockholm  ...      5504           8949\n","3      Arthur Kegels    BRUSSELS  ...      8999            300\n","4      Freya Bismark      Berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"VcZ63_4lWDRG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618417437347,"user_tz":-180,"elapsed":2187,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"477f4ba3-6f8b-4984-d823-410822535393"},"source":["# Example 1b\n","# ---\n","# Alternatively, we can rename column names in a dataframe manually by \n","# specifying new column names that would replace the original column names.\n","# You should note that this method is cumbersome when \n","# the no. of features/varibles/columns become large.\n","# ---\n","#\n","\n","# To demonstrate this, we will need to import our dataset again \n","# so that we work with our original dataset. \n","# ---\n","# \n","df = pd.read_csv('http://bit.ly/DataCleaningDataset', ';') \n","\n","# We then specify our columns names, store them in a list, then afterwards\n","# assign this list to the original column names. \n","\n","# Lets first see our original column names below\n","# ---\n","# \n","df.columns"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['NAME', 'CITY', 'COUNTRY', 'HEIGHT', 'WEIGHT', 'ACCOUNT A', 'ACCOUNT B',\n","       'TOTAL ACCOUNT'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"cFccEuW9SVGY","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618417450312,"user_tz":-180,"elapsed":1236,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"e245dc1d-8d95-40eb-96bf-39773819c460"},"source":["# We then perform our column values replacement as shown below\n","# ---\n","# \n","df.columns = ['name', 'city', 'country', 'height', 'weight', 'account_a', 'account_b', 'total_account']\n","\n","# We then preview our dataframe as shown \n","# ---\n","#\n","df.head()"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>account_a</th>\n","      <th>account_b</th>\n","      <th>total_account</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>LISBON</td>\n","      <td>PORTUGAL</td>\n","      <td>56</td>\n","      <td>132.0</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>LONDON</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>62</td>\n","      <td>165.0</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>Stockholm</td>\n","      <td>Sweden</td>\n","      <td>48</td>\n","      <td>117.0</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>BRUSSELS</td>\n","      <td>BELGIUM</td>\n","      <td>59</td>\n","      <td>121.0</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>Berlin</td>\n","      <td>GERMANYY</td>\n","      <td>53</td>\n","      <td>126.0</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                name        city  ... account_b  total_account\n","0          Adi Dako   LISBON      ...      4340           6730\n","1          John Paul   LONDON     ...     34334          38834\n","2        Cindy Jules   Stockholm  ...      5504           8949\n","3      Arthur Kegels    BRUSSELS  ...      8999            300\n","4      Freya Bismark      Berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"7g_2_fG1md0X","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618417461565,"user_tz":-180,"elapsed":1448,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"1fce9f59-bb81-41a0-fc25-fcdd8de8438c"},"source":["# Example 2\n","# ---\n","# We can also perform string conversion to a particular column. \n","# This allows us to have uniformity across all values of a column.\n","# In this example, we will convert the values of the column \"city\" to lower case values.\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","# Lets convert the city column to comprise of only lowercase characters\n","# ---\n","# \n","df['city'] = df['city'].str.lower()\n","df.head()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>account_a</th>\n","      <th>account_b</th>\n","      <th>total_account</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>lisbon</td>\n","      <td>PORTUGAL</td>\n","      <td>56</td>\n","      <td>132.0</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>london</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>62</td>\n","      <td>165.0</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>stockholm</td>\n","      <td>Sweden</td>\n","      <td>48</td>\n","      <td>117.0</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>brussels</td>\n","      <td>BELGIUM</td>\n","      <td>59</td>\n","      <td>121.0</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>berlin</td>\n","      <td>GERMANYY</td>\n","      <td>53</td>\n","      <td>126.0</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                name        city  ... account_b  total_account\n","0          Adi Dako   lisbon      ...      4340           6730\n","1          John Paul   london     ...     34334          38834\n","2        Cindy Jules   stockholm  ...      5504           8949\n","3      Arthur Kegels    brussels  ...      8999            300\n","4      Freya Bismark      berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"F8bgFBh2JAs3","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618417494629,"user_tz":-180,"elapsed":1301,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"6bf2d628-c963-4fdc-ecfe-b26b068a318a"},"source":["# Example 3\n","# ---\n","# We can also perform types of conversion that we would want i.e. metric conversion.\n","# In this example, we will convert our height values to centimeters noting \n","# that 1 inch = 2.54 cm.\n","# ---\n","# Dataset url = http://bit.ly/DataCleaningDataset\n","# ---\n","# \n","\n","# We can perform our conversion across the column that we would want \n","# then replace the column with the outcome of our conversion.\n","# ---\n","#\n","df['height'] = df['height'] * 2.54\n","df.head()"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>account_a</th>\n","      <th>account_b</th>\n","      <th>total_account</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>lisbon</td>\n","      <td>PORTUGAL</td>\n","      <td>142.24</td>\n","      <td>132.0</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>london</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>157.48</td>\n","      <td>165.0</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>stockholm</td>\n","      <td>Sweden</td>\n","      <td>121.92</td>\n","      <td>117.0</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>brussels</td>\n","      <td>BELGIUM</td>\n","      <td>149.86</td>\n","      <td>121.0</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>berlin</td>\n","      <td>GERMANYY</td>\n","      <td>134.62</td>\n","      <td>126.0</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                name        city  ... account_b  total_account\n","0          Adi Dako   lisbon      ...      4340           6730\n","1          John Paul   london     ...     34334          38834\n","2        Cindy Jules   stockholm  ...      5504           8949\n","3      Arthur Kegels    brussels  ...      8999            300\n","4      Freya Bismark      berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"l7xqz-EHilp9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618417502748,"user_tz":-180,"elapsed":1123,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"018ce63e-2d21-44e9-f96c-87088cba9af0"},"source":["# Example 4\n","# ---\n","# We can also perform other types of conversion such as datatype conversion as shown\n","# in the next cell.\n","# ---\n","\n","\n","# But before we do that, let's first determine the column/feature datatypes\n","# ---\n","# \n","df.dtypes"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["name              object\n","city              object\n","country           object\n","height           float64\n","weight           float64\n","account_a        float64\n","account_b          int64\n","total_account      int64\n","dtype: object"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"MVwcfVsfjW7G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618418115122,"user_tz":-180,"elapsed":2042,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"05800efd-56b1-4121-96f4-9e58f43e0bab"},"source":["# Then perform a conversion by converting our column/feature \n","# through the use of the apply() function, passing the numerical \n","# type provided by numpy.\n","# To get an understanding of other datatypes provided by numpy we can visit: \n","# https://docs.scipy.org/doc/numpy/user/basics.types.html\n","# ---\n","# Other \n","# ---\n","# \n","import numpy as np\n","\n","\n","df['height'] = df['height'].apply(np.int64)\n","\n","# Let's now check whether our conversion happened by checking our updated datatypes\n","# ---\n","# \n","df.dtypes"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["name              object\n","city              object\n","country           object\n","height             int64\n","weight           float64\n","account_a        float64\n","account_b          int64\n","total_account      int64\n","dtype: object"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"2gjU-yrmmo8I"},"source":["#### <font color=\"green\">Challenges</font> "]},{"cell_type":"code","metadata":{"id":"M81HSax-W3-T","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618418152150,"user_tz":-180,"elapsed":2091,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"743fc544-b389-4995-dd99-bb6b28468c27"},"source":["# Challenge 1\n","# ---\n","# Question: Convert the variables account_a and account_b to integer datatype.\n","# ---\n","# Hint: You can refer to the df dataframe in the example. \n","# ---\n","# OUR CODE GOES BELOW\n","#\n","import numpy as np\n","\n","df['account_b'] = df['account_b'].apply(np.int64)\n","df.head()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>account_a</th>\n","      <th>account_b</th>\n","      <th>total_account</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>lisbon</td>\n","      <td>PORTUGAL</td>\n","      <td>142</td>\n","      <td>132.0</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>london</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>157</td>\n","      <td>165.0</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>stockholm</td>\n","      <td>Sweden</td>\n","      <td>121</td>\n","      <td>117.0</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>brussels</td>\n","      <td>BELGIUM</td>\n","      <td>149</td>\n","      <td>121.0</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>berlin</td>\n","      <td>GERMANYY</td>\n","      <td>134</td>\n","      <td>126.0</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                name        city  ... account_b  total_account\n","0          Adi Dako   lisbon      ...      4340           6730\n","1          John Paul   london     ...     34334          38834\n","2        Cindy Jules   stockholm  ...      5504           8949\n","3      Arthur Kegels    brussels  ...      8999            300\n","4      Freya Bismark      berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"UGSOBG7im9Ch","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618418239380,"user_tz":-180,"elapsed":1124,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"30dc0d5c-a0c3-4ab1-e93e-8fa56493e6d2"},"source":["# Challenge 2\n","# ---\n","# Question: Convert the given weight feature in the dataset from pounds to grams. \n","# ---\n","# Hint: 1 pound = 453.592 grams \n","#     : You can refer to the df dataframe in the example. \n","# ---\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","#import numpy as np\n","\n","df['weight'] = df['weight'] * 453.592 \n","df.head()"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>account_a</th>\n","      <th>account_b</th>\n","      <th>total_account</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>lisbon</td>\n","      <td>PORTUGAL</td>\n","      <td>142</td>\n","      <td>59874.144</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>london</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>157</td>\n","      <td>74842.680</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>stockholm</td>\n","      <td>Sweden</td>\n","      <td>121</td>\n","      <td>53070.264</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>brussels</td>\n","      <td>BELGIUM</td>\n","      <td>149</td>\n","      <td>54884.632</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>berlin</td>\n","      <td>GERMANYY</td>\n","      <td>134</td>\n","      <td>57152.592</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                name        city  ... account_b  total_account\n","0          Adi Dako   lisbon      ...      4340           6730\n","1          John Paul   london     ...     34334          38834\n","2        Cindy Jules   stockholm  ...      5504           8949\n","3      Arthur Kegels    brussels  ...      8999            300\n","4      Freya Bismark      berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"OWcy51lCOuTv"},"source":["## 4. Syntax Errors"]},{"cell_type":"markdown","metadata":{"id":"ONPG_rNpOuT8"},"source":["#### <font color=\"blue\">Examples</font>"]},{"cell_type":"code","metadata":{"id":"6JgjmQlAOuUA","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618418328912,"user_tz":-180,"elapsed":5003,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"2306c55d-b93d-4001-9477-571938655807"},"source":["# Example 1\n","# --- \n","# While performing our analysis, we can get to a point where we need to \n","# fix spelling mistakes or typos. This example will show us how we can \n","# go about this.\n","# ---\n","# Dataset url = http://bit.ly/DataCleaningDataset\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","# Let's replacing any value \"GERMANYY\" with the correct value \"GERMANY\".\n","# We use the string replace() function to perform our operation as shown.\n","# ---\n","# \n","df['country'] = df['country'].str.replace('GERMANYY', 'GERMANY')\n","df.head()"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>city</th>\n","      <th>country</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>account_a</th>\n","      <th>account_b</th>\n","      <th>total_account</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Adi Dako</td>\n","      <td>lisbon</td>\n","      <td>PORTUGAL</td>\n","      <td>142</td>\n","      <td>59874.144</td>\n","      <td>2390.0</td>\n","      <td>4340</td>\n","      <td>6730</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>John Paul</td>\n","      <td>london</td>\n","      <td>UNITED KINGDOM</td>\n","      <td>157</td>\n","      <td>74842.680</td>\n","      <td>4500.0</td>\n","      <td>34334</td>\n","      <td>38834</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Cindy Jules</td>\n","      <td>stockholm</td>\n","      <td>Sweden</td>\n","      <td>121</td>\n","      <td>53070.264</td>\n","      <td>NaN</td>\n","      <td>5504</td>\n","      <td>8949</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arthur Kegels</td>\n","      <td>brussels</td>\n","      <td>BELGIUM</td>\n","      <td>149</td>\n","      <td>54884.632</td>\n","      <td>4344.0</td>\n","      <td>8999</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Freya Bismark</td>\n","      <td>berlin</td>\n","      <td>GERMANY</td>\n","      <td>134</td>\n","      <td>57152.592</td>\n","      <td>7000.0</td>\n","      <td>19000</td>\n","      <td>26000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                name        city  ... account_b  total_account\n","0          Adi Dako   lisbon      ...      4340           6730\n","1          John Paul   london     ...     34334          38834\n","2        Cindy Jules   stockholm  ...      5504           8949\n","3      Arthur Kegels    brussels  ...      8999            300\n","4      Freya Bismark      berlin  ...     19000          26000\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"cViv8asNOuUM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618418348994,"user_tz":-180,"elapsed":2334,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"82684ffe-6267-4c31-be12-73f1b1bc595e"},"source":["# Example 2\n","# ---\n","# We can also decide to strip or remove leading spaces (space infront) \n","# and trailing spaces (spaces at the end) by using the string strip() function\n","# covered in this example.\n","# ---\n","# Dataset = http://bit.ly/DataCleaningDataset\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","# We first load our dataframe column with the intention to observing leading \n","# and trailing spaces in the city column\n","# ---\n","# \n","df['city']"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0           lisbon    \n","1            london   \n","2            stockholm\n","3             brussels\n","4               berlin\n","5         brasilia    \n","6            stockholm\n","7            london   \n","Name: city, dtype: object"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"u6hu9lJXnpPs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618418409890,"user_tz":-180,"elapsed":5049,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"671afbb0-a049-4960-b07e-d5cbc1f824dd"},"source":["# Then later we strip the leading and trailing spaces as shown and lastly \n","# confirm our changes  \n","# ---\n","# \n","df['city'] = df['city'].str.strip()\n","df['city']"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       lisbon\n","1       london\n","2    stockholm\n","3     brussels\n","4       berlin\n","5     brasilia\n","6    stockholm\n","7       london\n","Name: city, dtype: object"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"zgSTFXdcOuUc"},"source":["#### <font color=\"green\">Challenges</font> "]},{"cell_type":"code","metadata":{"id":"N7vIG505OuUo","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1618420084475,"user_tz":-180,"elapsed":2756,"user":{"displayName":"Kuria K. Ken","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnEdqxk2Mp_jIE4QawUovTRuGsreOMU5LcwWtLIw=s64","userId":"18249265223060828810"}},"outputId":"20562c05-0d90-444b-852f-cb159a3ece29"},"source":["# Challenge 1\n","# ---\n","# Question: Deal with the leading and trailing whitespaces from Name \n","# and Team variables in the following dataset.\n","# ---\n","# Dataset url = http://bit.ly/NBABasketballDataset\n","# ---\n","# OUR CODE GOES BELOW\n","#  \n","df = pd.read_csv('http://bit.ly/NBABasketballDataset', ',') \n","\n","#df.columns = ['name', 'NBA Team', 'Number', 'Position', 'Age', 'Height', 'Weight', 'College', 'Salary']\n","#df['Name'] = df['Name'].str.strip()\n","df.head()"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Team</th>\n","      <th>Number</th>\n","      <th>Position</th>\n","      <th>Age</th>\n","      <th>Height</th>\n","      <th>Weight</th>\n","      <th>College</th>\n","      <th>Salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Avery Bradley</td>\n","      <td>Boston Celtics</td>\n","      <td>0.0</td>\n","      <td>PG</td>\n","      <td>25.0</td>\n","      <td>6-2</td>\n","      <td>180.0</td>\n","      <td>Texas</td>\n","      <td>7730337.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Jae Crowder</td>\n","      <td>Boston Celtics</td>\n","      <td>99.0</td>\n","      <td>SF</td>\n","      <td>25.0</td>\n","      <td>6-6</td>\n","      <td>235.0</td>\n","      <td>Marquette</td>\n","      <td>6796117.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>John Holland</td>\n","      <td>Boston Celtics</td>\n","      <td>30.0</td>\n","      <td>SG</td>\n","      <td>27.0</td>\n","      <td>6-5</td>\n","      <td>205.0</td>\n","      <td>Boston University</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>R.J. Hunter</td>\n","      <td>oston Celtics</td>\n","      <td>28.0</td>\n","      <td>SG</td>\n","      <td>22.0</td>\n","      <td>6-5</td>\n","      <td>185.0</td>\n","      <td>Georgia State</td>\n","      <td>1148640.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Jonas Jerebko</td>\n","      <td>Boston Celtics</td>\n","      <td>8.0</td>\n","      <td>PF</td>\n","      <td>29.0</td>\n","      <td>6-10</td>\n","      <td>231.0</td>\n","      <td>NaN</td>\n","      <td>5000000.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Name            Team  Number  ... Weight            College     Salary\n","0  Avery Bradley  Boston Celtics     0.0  ...  180.0              Texas  7730337.0\n","1    Jae Crowder  Boston Celtics    99.0  ...  235.0          Marquette  6796117.0\n","2   John Holland  Boston Celtics    30.0  ...  205.0  Boston University        NaN\n","3    R.J. Hunter   oston Celtics    28.0  ...  185.0      Georgia State  1148640.0\n","4  Jonas Jerebko  Boston Celtics     8.0  ...  231.0                NaN  5000000.0\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"ebaricL9O20B"},"source":["## 5. Irrelevant Data"]},{"cell_type":"markdown","metadata":{"id":"WWZ0GCbCO20L"},"source":["#### <font color=\"blue\">Examples</font>"]},{"cell_type":"code","metadata":{"id":"cTzUuFAyO20Q"},"source":["# Example 1\n","# --- \n","# We can also deleting/dropping irrelevant columns/features. \n","# By irrelevant we mean dataset features that we don't need \n","# to answer a research question. \n","# ---\n","# Dataset url = http://bit.ly/DataCleaningDataset\n","# Hint: Use the df dataframe you created earlier\n","# ---\n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oiYYzezq4Wr"},"source":["# Deleting an Irrelevant Column i.e. if we didn't require the column city \n","# to answer our research question.\n","# ---\n","# While dropping/deleting those two columns:\n","# a) We set axis = 1\n","#    A dataframe has two axes: “axis 0” and “axis 1”. \n","#    “axis 0” represents rows and “axis 1” represents columns.\n","# b) We can also set Inplace = True.\n","#    This means the changes would be made to the original dataframe.\n","# Dropping the irrelevant columns i.e. Team and Weight\n","# Those values were dropped since axis was set equal to 1 and \n","# the changes were made in the original data frame since inplace was True.\n","# \n","df.drop([\"city\"], axis = 1, inplace = True) \n"," \n","# And preview our resulting dataset\n","# ---\n","# \n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"naAOHnkNsz-y"},"source":["# We can drop multiple columns as shown\n","# ---\n","# \n","df.drop([\"height\", \"weight\"], axis = 1, inplace = True) \n","\n","# And preview our resulting dataset \n","# --- \n","# \n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-TlGDqmxO20t"},"source":["# Example 2\n","# ---\n","# We can also fix in-record & cross-datasets errors. \n","# These kinds errors result from having two or more values in the same row \n","# or across datasets contradicting with each other.\n","# ---\n","# Dataset = http://bit.ly/DataCleaningDataset\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","df['total_account_2'] = df['account_a'] + df['account_b']\n","\n","# Previewing our resulting dataframe \n","# ---\n","#\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zf0W-6fnvx0A"},"source":["# Create another column to tell us whether if the two columns match.\n","# We will use the numpy library through use of np.\n","# ---\n","# \n","df['total_account?'] = np.where(df['total_account'] == df['total_account_2'], 'True', 'False')\n","\n","# Previewing our resulting dataframe \n","# ---\n","# \n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGxUgJJ4wilM"},"source":["# Let's now select the records which don't match \n","# ---\n","# \n","df.loc[df['total_account?'] == \"False\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CeKrayqxQuL"},"source":["# At this point we can do several things\n","# 1. Correct the values,\n","# 2. Drop/Delete the values,\n","# 3. Or even decide to leave them as they are for certain reasons\n","# ---\n","# If we had a large dataset, we could get the no. of records using len(),\n","# this would help us in our decision making process.\n","# ---\n","# \n","len(df.loc[df['total_account?'] == \"False\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXsiMoefO21J"},"source":["#### <font color=\"green\">Challenges</font> "]},{"cell_type":"code","metadata":{"id":"18qYASsPO21L"},"source":["# Challenge 1\n","# ---\n","# Question: While perfoming some analysis to answer a research question, \n","# we realize that we don't need the Date and Time features in our dataset. \n","# Let's drop those two features below\n","# ---\n","# Dataset url = https://bit.ly/SuperMarketSalesDB\n","# ---\n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQvS83K_PBJh"},"source":["## 6. Duplicates"]},{"cell_type":"markdown","metadata":{"id":"CjklXwZLPBJw"},"source":["#### <font color=\"blue\">Examples</font>"]},{"cell_type":"code","metadata":{"id":"dTlrpqh-PBJ5"},"source":["# Example 1\n","# --- \n","# Finding duplicate records\n","# -> Duplicate records are repeated records in a dataset.\n","# ---\n","# Dataset url = http://bit.ly/NBABasketballDataset\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","nba_df = pd.read_csv('http://bit.ly/NBABasketballDataset')\n"," \n","# Again, we first explore our dataset by determining the shape of \n","# our dataset (records/instances, columns/variables)\n","# ---\n","# \n","nba_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRdHqs9p1L8t"},"source":["# We can then identify which observations are duplicates\n","# through the duplicated() function and sum() to know how many\n","# duplicate records there are.\n","# Normally, duplicate records are dropped from the dataset.\n","# But in our case we don't have any duplicate records.\n","# ---\n","#\n","nba_df = nba_df[~nba_df.duplicated()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kd6R2AB77A1e"},"source":["# Finding the no. of duplicates\n","# ---\n","# \n","sum(nba_df.duplicated())  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFMDOQUCPBKg"},"source":["# Example 2\n","# ---\n","# Dropping duplicate columns\n","# ---\n","# Dataset = http://bit.ly/DataCleaningDataset\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","# In our previous dataset, if there were duplicates we\n","# could have dropped the through the use of the drop_duplicates() function \n","# as shown in this example \n","# ---\n","# \n","nba_df_duplicates = nba_df.drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CNl8VL-PBKp"},"source":["# Example 3\n","# ---\n","# Dropping duplicates in a specific column in thed df dataframe\n","# ---\n","# Dataset url = http://bit.ly/DataCleaningDataset\n","# ---\n","#\n","\n","# We can also consider records with repeated variables/columns \n","# as duplicates and deal with them. For example, we can \n","# identify duplicates in our dataset based on country.\n","# ---\n","#  \n","duplicates_df = df[df.duplicated(['country'])] \n","duplicates_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D33CBMxu5agd"},"source":["# Then dropping the duplicates as shown below.\n","# NB: We will create in a new dataframe object which will contain our unique dataframe\n","# which won't have any duplicates.\n","# --- \n","# \n","unique_df = df.drop_duplicates(['country'])\n","\n","# Determining the size of our new dataset \n","# We note that the two records were dropped from our original dataset\n","# ---\n","# \n","unique_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qP7c_QZcPBK0"},"source":["#### <font color=\"green\">Challenges</font> "]},{"cell_type":"code","metadata":{"id":"1qxMH94BPBK4"},"source":["# Challenge 1\n","# ---\n","# Question: Find the duplicates in the following dataset.\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS \n","# ---\n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ln3Js0AFPBLC"},"source":["# Challenge 2\n","# ---\n","# Question: From your understanding of the features, deal with the duplicates found in the given dataset.\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---\n","# OUR CODE GOES BELOW\n","# \n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jmQzRWnBAUip"},"source":["## 7. Missing Data"]},{"cell_type":"markdown","metadata":{"id":"RVDxacbDW-Wo"},"source":["#### <font color=\"blue\">Examples</font>"]},{"cell_type":"code","metadata":{"id":"d2e_4DahAZ1L"},"source":["# Example 3a\n","# ---\n","# Checking for missing data\n","# NB: This method may not be the most convenient. Why? \n","# --- \n","# We can check if there is any missing values in the entire dataframe as shown \n","# ---\n","# OUR CODE GOES BELOW\n","# \n","df.isnull()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lzLGQemeAZtn"},"source":["# Example 3b\n","# --- \n","# We can also check for missing values in each column \n","# ---\n","# OUR CODE GOES BELOW\n","#\n","df.isnull().any()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1q6GrIVILiQb"},"source":["# Example 3c\n","# ---\n","# We can check how many missing values there are across each column by \n","# ---\n","# OUR CODE GOES BELOW\n","#\n","df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6c26TzAbLljF"},"source":["# Example 3d\n","# ---\n","# We can also check to see if we have any missing values in the dataframe  \n","# ---\n","# OUR CODE GOES BELOW\n","#\n","print(df.isnull().values.any())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_qoob_9LnZP"},"source":["# Example 3e: Method 1\n","# ---\n","# Dealing with the missing data\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","# We can drop the missing observations \n","# ---\n","#\n","df_no_missing = df.dropna()\n","\n","# Checking for missing data\n","df_no_missing.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cg4-ef-sLqlb"},"source":["# Example 3e: Method 2\n","# ---\n","# We can drop rows where all cells in that row is NA\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","df_cleaned = df.dropna(how='all')\n","\n","# Checking the shape of our dataset\n","df_cleaned.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UxfcpIWILsHF"},"source":["# Example 3e: Method 3\n","# ---\n","# We could drop columns if they only contain missing values\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","df_without_columns = df.dropna(axis=1, how='all')\n","\n","# Checking the shape of our dataset\n","df_without_columns.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nRHn99xLuGU"},"source":["# Example 3e: Method 4\n","# ---\n","# We could drop rows that contain less than five observations\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","df.dropna(thresh=5)\n","\n","# Checking the shape of our dataset\n","df.shape\n","\n","\n","# Further reading\n","# ----\n","# Above are only a few methods of dealing with missing data. "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PxBhzro9WOsp"},"source":["#### <font color=\"green\">Challenges</font> "]},{"cell_type":"code","metadata":{"id":"LHVLpO6bLt9r"},"source":["# Challenge 3a\n","# ---\n","# Question: Find the missing values in the following datset.\n","# You can use any of the above methods to you see fit.\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---  \n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VIaciSKTMCRl"},"source":["# Challenge 3b\n","# ---\n","# Question: Handle the missing values in the following dataset.\n","# You can any of the above methods that you see fit.\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---\n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZOg8AbeAaub"},"source":["## 8. Filtering"]},{"cell_type":"markdown","metadata":{"id":"FFOkaq3vXZMh"},"source":["#### <font color=\"blue\">Examples & Challenges</font>"]},{"cell_type":"code","metadata":{"id":"hlWrf44NAecP"},"source":["# Example 4a\n","# ---\n","# Selecting rows when columns contain certain values\n","# ---\n","# NB: Selecting records where petal_length is 5.0 \n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","# Reading our csv file from the given url and storing it to a dataframe.\n","iris_df = pd.read_csv(\"http://bit.ly/IrisDataset\")\n","\n","# Previewing the first 5 records\n","iris_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CssurHCU2gXU"},"source":["# Selecting the records where column has certain values \n","# ---\n","#\n","iris_df[iris_df.petal_length.isin(['5.0'])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"48D2vBZuEukC"},"source":["# Challenge 4a\n","# ---\n","# Question: From the given dataset, find observations with outlets\n","# established in 2002.\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---\n","# OUR CODE GOES BELOW\n","# \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQoQq-nfAjL7"},"source":["# Example 4b\n","# ---\n","# Selecting the records where column doesn't have certain values \n","# in our case, where petal_leghth is not 5.0\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","iris_df[~iris_df.petal_length.isin(['5.0'])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5IPcb_bEv_B"},"source":["# Challenge 4b\n","# ---\n","# Question: Select all the Dairy observations from the given dataset.\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---\n","# OUR CODE GOES BELOW\n","# \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDnwO9TSHy3j"},"source":["# Example 4c\n","# ---\n","# Selecting records using filters where petal_width greater than 1.9\n","# ---\n","# OUR CODE GOES BELOW\n","# \n","\n","iris_df[(iris_df['petal_width'] > 1.9)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYeaoRAIH0vO"},"source":["# Challenge 4c\n","# ---\n","# Question: Which observations had items outlet sales greater than 2000?\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---\n","# OUR CODE GOES BELOW\n","# \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhrdWafVWXkj"},"source":["# Example 5d\n","# ---\n","# We can also use the query method to get for data where petal_width is equal to 1.0\n","# Once you run this cell, replace the equals operator == to with less than < \n","# or greater than > operators to see their applications as well.\n","# ---\n","# The parameter inplace makes changes in the original dataframe if True.\n","# We should also note the query method works if the column name doesn’t have any empty spaces.\n","# Hence the need replace blank spaces in our column names with '_'\n","# ---\n","#  \n","\n","# Let's filter our data\n","#\n","iris_df.query('petal_width == 1.9', inplace = True) \n","\n","# Previewing our dataset \n","iris_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxqszAhTZCWU"},"source":["# We can also perform multiple condition filtering as shown below\n","# ---\n","#\n","\n","iris_df.query('sepal_width == 2.7 and petal_length == 5.1', inplace = True) \n","\n","# Previewing our dataset \n","iris_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ngs4-KCdszu"},"source":["# We can also get the same result as shown below \n","# ---\n","# \n","# Or uncomment the following line\n","iris_df[(iris_df.sepal_length == 5.8)  (iris_df.petal_length == 5.1)]\n","\n","# Previewing our dataset \n","iris_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrpXmqvbd_mo"},"source":["# Challenge 5d\n","# ---\n","# Question: Which observations had items outlet sales greater than 2000 and less than 3000?\n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---\n","# OUR CODE GOES BELOW\n","# \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-1tL9P5BAhWS"},"source":["## 9. Sorting"]},{"cell_type":"markdown","metadata":{"id":"-DeWc_D7WVBl"},"source":["#### <font color=\"blue\">Examples</font>"]},{"cell_type":"code","metadata":{"id":"QgC9N1_DAjq3"},"source":["# Example 5a\n","# --- \n","# Load the given dataframe in ascending order by reports \n","# ---\n","# OUR CODE GOES BELOW\n","# \n"," \n","# Sorting our dataset in ascending order by sepal_length\n","# ---\n","# \n","iris_df.sort_values(by='sepal_length', ascending=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVTVl_GpWToR"},"source":["#### <font color=\"green\">Challenges</font> "]},{"cell_type":"code","metadata":{"id":"too5E8HVBJSk"},"source":["# Challenge 5a \n","# --- \n","# Sort items by weight in descending order given the following dataset.\n","# Hint: ascending = 0 \n","# ---\n","# Dataset url = https://bit.ly/ShoprityDS\n","# ---\n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bISArgNWBxR-"},"source":["## 10. Splitting, Merging and Concatenation"]},{"cell_type":"markdown","metadata":{"id":"Hf72T4GKXzb_"},"source":["#### <font color=\"blue\">Examples & Challenges</font>"]},{"cell_type":"code","metadata":{"id":"zceIjjUiBy5A"},"source":["# Example 6a\n","# --- \n","# Split the dataframe species column into two columns by using the \"-\" character\n","# ---\n","# OUR CODE GOES BELOW\n","# \n"," \n","# Dropping null value columns to avoid errors \n","iris_df.dropna(inplace = True) \n","  \n","# New data frame with split value columns \n","new_iris_df = iris_df[\"species\"].str.split(\"-\", n = 1, expand = True) \n","  \n","# Naking separate first name column from new data frame \n","iris_df[\"family\"]= new_iris_df[0] \n","  \n","# Making separate last name column from new data frame \n","iris_df[\"sub-species\"]= new_iris_df[1] \n","  \n","# Dropping old Name columns \n","iris_df.drop(columns =[\"species\"], inplace = True) \n","  \n","# Displaying our dataframe\n","iris_df "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwGSNkqz02qp"},"source":["# Example 2\n","# ---\n","# Concatenating (merging) two columns in a dataframe\n","# ---  \n","# OUR CODE GOES BELOW\n","# \n","\n","# Concatenating our column\n","# \n","iris_df['species'] = iris_df['family'].str.cat(iris_df['sub-species'],sep=\"-\")\n","\n","# Dropping old name columns \n","#\n","iris_df.drop(columns = [\"family\", \"sub-species\"], inplace = True) \n","\n","# Previewing our new dataframe\n","#\n","iris_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEZyhikz3con"},"source":["# Challenge 6a\n","# --- \n","# Concatenate the the city and state columns from the dataset below. \n","# The resulting region column should have city and state seperated by a comma and whitespace.\n","# ---\n","# Dataset url = http://bit.ly/SchoolShootingsDataset\n","# ---\n","# Step 1: Load the dataset\n","# Step 2: Preview the dataset \n","# Step 3: Perform your concatenation\n","# Step 4: Dropping old columns\n","# Step 4: Preview your final dataframe\n","# ---\n","# OUR CODE GOES BELOW\n","# \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzPzBmYTPaSC"},"source":["# Example 6b\n","# ---\n","# Merging two dataframes. \n","# In this example, we will create two simple dataframes for demonstration purposes.\n","# ---  \n","# OUR CODE GOES BELOW\n","# \n","\n","# Let's create our first dataframe and preview it\n","# \n","df1 = pd.DataFrame([[2, 3], [41, 51]], columns=['a', 'b'])\n","df1.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"di8xE4ToPb7_"},"source":[" # Let our second dataframe and preview it\n","#\n","df2 = pd.DataFrame([[2, 5], [41, 6]], columns=['a', 'c'])\n","df2.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQ8fDmmkzhEl"},"source":["# Merging our dataframes and storing our resulting dataframe in df3, then previewing it \n","# \n","df3 = df1.merge(df2, how='left', on='a') \n","df3.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gSVuIQojFFBR"},"source":["# Challenge 6b\n","# --- \n","# Merge the following two datasets\n","# ---\n","# Dataset 1 url = http://bit.ly/CitiesDataset\n","# Dataset 2 url = http://bit.ly/CountriesDataset1\n","# ---\n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v5M3iGK8PZ3Y"},"source":["## 11. Outliers"]},{"cell_type":"markdown","metadata":{"id":"jnxNSUhdPZ3u"},"source":["#### <font color=\"blue\">Examples</font>"]},{"cell_type":"code","metadata":{"id":"NzpMrSX1PZ36"},"source":["# Example 1\n","# --- \n","# Given the following dataset, find and deal with outliers.\n","# ---\n","# Dataset url = http://bit.ly/CountryDataset1\n","# ---\n","# OUR CODE GOES BELOW\n","#  \n","\n","# Let's read data from url as dataframe\n","# \n","outliersc_df = pd.read_csv(\"http://bit.ly/CountryDataset1\") \n","\n","# Lets preview our our dataframe below\n","#\n","outliersc_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MtbaxstdIby"},"source":["# Checking the size of our dataset for cleaning purposes\n","# ---\n","#\n","outliersc_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q33K99Jsc1wH"},"source":["# There are many ways of dealing with the outliers however in this session we wiil \n","# use the interquartile range (IQR). \n","# The IQR is the first quartile subtracted from the third quartile, \n","# i.e. the range covered by the middle 50% of the data. Values outside this range\n","# will be considered as outliers. If we were to use a box plot visualisation then,\n","# we would be able to visually see those values outside this range. \n","# Something to note is that this method will consider only the numerical values in \n","# our dataset. Lets now calculate the IQR for each column.\n","# ---\n","#\n","\n","# We first defining our quantiles using the quantile() function\n","# ---\n","# \n","Q1 = outliersc_df.quantile(0.25)\n","Q3 = outliersc_df.quantile(0.75)\n","IQR = Q3 - Q1\n","IQR\n","\n","# Then filtering out our outliers by getting values which are outside our IQR Range.\n","# ---\n","#\n","outliers_df_iqr = outliersc_df[((outliersc_df < (Q1 - 1.5 * IQR)) | (outliersc_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n","\n","# Checking the size of the dataset with outliers for cleaning purposes\n","# ---\n","#\n","outliers_df_iqr.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWJth6qXczMC"},"source":["# We can also explore our outliers by doing the following\n","# ---\n","# \n","outliers_df_iqr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjGji1pYUMTq"},"source":["# Lastly, the most common method of handling our outliers is to drop them.\n","# In some cases we can:\n","# 1. Leave them if they are genuine\n","# 2. Replace them with values within the IQR range\n","# 3. Drop them\n","# ---\n","# In our case, we will drop them.\n","# We just use the \"~\" character to refer to the other part of the dataset that \n","# does not have outliers\n","# ---\n","# \n","clean_dfc_iqr = outliersc_df[ ~((outliersc_df < (Q1 - 1.5 * IQR)) | (outliersc_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n","\n","# Checking the size of our final dataset.\n","clean_dfc_iqr.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wt8Se1hXPZ4C"},"source":["#### <font color=\"green\">Challenges</font> "]},{"cell_type":"code","metadata":{"id":"hwsxAgwLPZ4Y"},"source":["# Challenge 1\n","# ---\n","# Question: Find and Deal with the outliers in the given df dataframe.\n","# ---\n","# Dataset url = http://bit.ly/DataCleaningDataset\n","# You can use the df dataframe you created ealier on.\n","# ---\n","# OUR CODE GOES BELOW\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAuIYctBB1ZX"},"source":["## 12. Exporting Data"]},{"cell_type":"markdown","metadata":{"id":"fJ84H5gZYIgZ"},"source":["#### <font color=\"blue\">Examples & Challenges</font>"]},{"cell_type":"code","metadata":{"id":"pMxOj6omB2a_"},"source":["# Pre-requisite 7\n","# ---\n","# Importing files module \n","# ---\n","# OUR CODE GOES BELOW\n","#  \n","\n","# Files module will allow us to download our file from colaboratory\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e83--Ka_B21z"},"source":["# Example 7a\n","# --- \n","# Load the given dataframe in ascending order by reports \n","# ---\n","# OUR CODE GOES BELOW\n","# \n","  \n","# Create our sample dataframe\n","data = {'name': ['Daniel', 'Joyce', 'Elizabeth', 'Sanni', 'Sefu'], \n","        'year': [2012, 2012, 2013, 2014, 2014], \n","        'reports': [4, 24, 31, 2, 3]}\n","df = pd.DataFrame(data, index = ['Nairobi', 'Cairo', 'Cape Town', 'Adis Ababa', 'Mombasa'])\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSqkbI0RPoq8"},"source":["# Exporting our sample dataframe as a csv\n","# ---\n","# \n"," \n","df.to_csv('example.csv') \n","\n","# NB: In order to download our files from colaboratory we need to run the following\n","#\n","files.download('example.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5RNQvkq3sww"},"source":["# Challenge 7a\n","# ---\n","# Question: Export a csv file of your resulting shoprity dataframe below \n","# --- \n","# OUR CODE GOES BELOW\n","#"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5_yqeq9Pq2h"},"source":["# Example 7b\n","# ---\n","# Exporting our sample dataframe as a excel file (xlsx)\n","# --- \n","# ---\n","# OUR CODE GOES BELOW\n","#  \n","\n","df.to_excel('example.xlsx') \n","\n","# NB: In order to download our files from colab we need to run the following\n","#\n","files.download('example.xlsx')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1DnZc-isbs9"},"source":["# Challenge 7b\n","# ---\n","# Question: Export a excel file of your resulting shoprity dataframe\n","# --- \n","# OUR CODE GOES BELOW\n","#\n"],"execution_count":null,"outputs":[]}]}